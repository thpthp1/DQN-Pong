# Making a Deep Q Learning model for pong

We are implementing this paper:://www.cs.toronto.edu/~vmnih/docs/dqn.pdf

xt âˆˆ R^d each of this is an image
Pseudocode of DQL with experience replay:
```
Initialize replay memory D to capacity N
Initialize action-value function Q with random weights
for episode = 1, M do
	Initialise sequence s_1 = {x_1} and preprocessed sequenced Ï†_1 = Ï†(s1)
	for t = 1, T do
		With probability  select a random action at
		otherwise select at = maxa Qâˆ—
		(Ï†(st), a; Î¸)
		Execute action at in emulator and observe reward rt and image xt+1
		Set s_{t+1} = s_t, a_t, x_{t+1} and preprocess Ï†t+1 = Ï†(st+1)
		Store transition (Ï†_t, a_t, rt, Ï†_{t+1}) in D
		Sample random minibatch of transitions (Ï†_j , aj , rj , Ï†_{j+1}) from D
		Set yj =
		
		rj for terminal Ï†_{j+1}
		rj + Î³ maxa0 Q(Ï†_{j+1}, a_0; Î¸) for non-terminal Ï†j+1
		Perform a gradient descent step on (yj âˆ’ Q(Ï†_j , a_j ; Î¸))2
		according to equation 3
	end for
end for
```
Class architecture

Agent:
	objects:
		- Model (tf.model)
		- gamma
		- state-memory (either to a memory list/stack or a text file)
		- Episodes
	functions:
		- train
		- replay
		- save_weights
		- play
